{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de CMI_Pytorch.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"id":"QUMtboSD2Lsk","outputId":"17e45673-291b-4656-9065-774639b1ee66","executionInfo":{"status":"ok","timestamp":1651020531466,"user_tz":-120,"elapsed":32755,"user":{"displayName":"Francisco Cardenas","userId":"06925703395123571947"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Notebooks/PP2 Mio\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/gdrive/MyDrive/Colab Notebooks/PP2 Mio'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["# pytorch mlp for multiclass classification\n","from numpy import vstack\n","from numpy import argmax\n","from pandas import read_csv\n","import pickle\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","from torch import Tensor\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split\n","from torch.nn import Linear\n","from torch.nn import ReLU\n","from torch.nn import Softmax\n","from torch.nn import Module\n","from torch.optim import SGD\n","from torch.nn import CrossEntropyLoss\n","from torch.nn.init import kaiming_uniform_\n","from torch.nn.init import xavier_uniform_\n","import sys\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from torchvision import transforms\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from torch.nn import BCELoss\n","import numpy as np\n","import seaborn as sns\n","import tensorflow as tf\n","import re\n","import string\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer \n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","import torch\n","from torch import nn\n","import scipy\n","from torch import optim\n","import time\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words = set(stopwords.words('english')) \n","\n","def MyCleanText(X, \n","               lowercase=False, # mettre en minuscule\n","               removestopwords=False, # supprimer les stopwords\n","               removedigit=False, # supprimer les nombres  \n","               getstemmer=False, # conserver la racine des termes\n","               getlemmatisation=False # lematisation des termes \n","              ):\n","    \n","    sentence=str(X)\n","\n","    # suppression des caractères spéciaux\n","    sentence = re.sub(r'[^\\w\\s]',' ', sentence)\n","    # suppression de tous les caractères uniques\n","    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n","    # substitution des espaces multiples par un seul espace\n","    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n","\n","    # decoupage en mots\n","    tokens = word_tokenize(sentence)\n","    if lowercase:\n","          tokens = [token.lower() for token in tokens]\n","\n","    # suppression ponctuation\n","    table = str.maketrans('', '', string.punctuation)\n","    words = [token.translate(table) for token in tokens]\n","\n","    # suppression des tokens non alphabetique ou numerique\n","    words = [word for word in words if word.isalnum()]\n","    \n","    # suppression des tokens numerique\n","    if removedigit:\n","        words = [word for word in words if not word.isdigit()]\n","\n","    # suppression des stopwords\n","    if removestopwords:\n","        words = [word for word in words if not word in stop_words]\n","\n","    # lemmatisation\n","    if getlemmatisation:\n","        lemmatizer=WordNetLemmatizer()\n","        words = [lemmatizer.lemmatize(word)for word in words]\n","        \n","\n","    # racinisation\n","    if getstemmer:\n","        ps = PorterStemmer()\n","        words=[ps.stem(word) for word in words]\n","        \n","    sentence= ' '.join(words)\n","  \n","    return sentence   \n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class TextNormalizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, \n","                 removestopwords=False, # suppression des stopwords\n","                 lowercase=False,# passage en minuscule\n","                 removedigit=False, # supprimer les nombres  \n","                 getstemmer=False,# racinisation des termes \n","                 getlemmatisation=False # lemmatisation des termes  \n","                ):\n","        \n","        self.lowercase=lowercase\n","        self.getstemmer=getstemmer\n","        self.removestopwords=removestopwords\n","        self.getlemmatisation=getlemmatisation\n","        self.removedigit=removedigit\n","        #self.transform = transforms.Compose([transforms.ToTensor()])  \n","\n","    def transform(self, X, **transform_params):\n","        # Nettoyage du texte\n","        X=X.copy() # pour conserver le fichier d'origine\n","        return [MyCleanText(text,lowercase=self.lowercase,\n","                            getstemmer=self.getstemmer,\n","                            removestopwords=self.removestopwords,\n","                            getlemmatisation=self.getlemmatisation,\n","                            removedigit=self.removedigit) for text in X]\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","    \n","    def fit_transform(self, X, y=None, **fit_params):\n","        return self.fit(X).transform(X)\n","\n","    def get_params(self, deep=True):\n","        return {\n","            'lowercase':self.lowercase,\n","            'getstemmer':self.getstemmer,\n","            'removestopwords':self.removestopwords,\n","            'getlemmatisation':self.getlemmatisation,\n","            'removedigit':self.removedigit\n","        }    \n","    \n","    def set_params (self, **parameters):\n","        for parameter, value in parameters.items():\n","            setattr(self,parameter,value)\n","        return self    \n","        \n","        pipe = Pipeline([(\"cleaner\", TextNormalizer(lowercase=True)),\n","                 (\"tfidf_vectorizer\", TfidfVectorizer(max_features=NB_FEATURES))])\n","\n","        #pipe.fit(self.X)\n","        #self.X=pipe.transform(self.X)\n","        self.X=pipe.fit_transform(self.X)\n","        # sauvegarde du vocabulaire du TF_IDF pour tester d'autres données\n","        print (pipe.named_steps['tfidfvectorizer'].vocabulary)\n","        #corpus  = [tokenize(doc) for doc in self.X]\n","        #lexicon = gensim.corpora.Dictionary(corpus)\n","        #lexicon.save_as_text('lexicon.txt', sort_by_word=True)\n","        pickle.dump(pipe.named_steps['tfidfvectorizer'].vocabulary,open(\"feature.pkl\",\"wb\"))\n","\n","        self.X=self.X.toarray()\n","        self.X = self.X.astype('float32')\n","        self.y = LabelEncoder().fit_transform(self.y)\n","\n","\n","# pour monter son drive Google Drive local\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","my_local_drive='/content/gdrive/MyDrive/Colab Notebooks/PP2 Mio'\n","# Ajout du path pour les librairies, fonctions et données\n","sys.path.append(my_local_drive)\n","# Se positionner sur le répertoire associé\n","%cd $my_local_drive\n","\n","%pwd\n"]},{"cell_type":"code","source":["\n","NB_FEATURES=1000\n","EPOCHS=150\n","\n","\n","\n","# dataset definition\n","class CSVDataset(Dataset):\n","    # load the dataset\n","    def __init__(self, path):\n","        # load the csv file as a dataframe\n","        df = read_csv(path, header=None, sep='\\t', encoding='utf8')\n","        #print(df.head())\n","        # store the inputs and outputs\n","        self.X = df.values[:, :-1]\n","\n","        self.y = df.values[:, -1]\n","\n","        # création du pipeline\n","        pipe = Pipeline([(\"cleaner\", TextNormalizer(lowercase=True)),\n","                 (\"tfidf_vectorizer\", TfidfVectorizer(max_features=NB_FEATURES))])\n","\n","\n","        self.X=pipe.fit_transform(self.X)\n","        # sauvegarde du vocabulaire du TF_IDF pour tester d'autres données\n","        pickle.dump(pipe.named_steps['tfidf_vectorizer'].vocabulary_,open(\"vocabulaire.txt\",\"wb\"))\n","\n","        # attention important il faut convertir le résultat de TF_IDF en array et dire que dedans ce \n","        # sont des floats\n","        self.X=self.X.toarray()\n","        self.X = self.X.astype('float32')\n","        # ne pas oublier de transformer le y via label encoder -> chaque classe aura un numéro\n","        self.y = LabelEncoder().fit_transform(self.y)\n","\n","\n","    # number of rows in the dataset\n","    def __len__(self):\n","        return len(self.X)\n"," \n","    # get a row at an index\n","    def __getitem__(self, idx):\n","        return [self.X[idx], self.y[idx]]\n","       # return self.transform(self.x_data[idx]), self.transform(self.y_data[idx])\n"," \n","    # get indexes for train and test rows\n","    def get_splits(self, n_test=0.33):\n","        # determine sizes\n","        test_size = round(n_test * len(self.X))\n","        train_size = len(self.X) - test_size\n","        # calculate the split\n","        return random_split(self, [train_size, test_size])\n","\n","    \n","\n","# model definition\n","class MLP(Module):\n","    # define model elements\n","    def __init__(self, n_inputs):\n","        super(MLP, self).__init__()\n","        # input to first hidden layer\n","        self.hidden1 = Linear(n_inputs, 10)\n","        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n","        self.act1 = ReLU()\n","        # second hidden layer\n","        self.hidden2 = Linear(10, 8)\n","        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n","        self.act2 = ReLU()\n","        # third hidden layer and output\n","        #self.hidden3 = Linear(8, 3)\n","        self.hidden3 = Linear(8, 4)\n","        xavier_uniform_(self.hidden3.weight)\n","        #self.act3 = Softmax(dim=1)\n","        self.act3 = Softmax(dim=1)\n"," \n","    # forward propagate input\n","    def forward(self, X):\n","        # input to first hidden layer\n","        X = self.hidden1(X)\n","        X = self.act1(X)\n","        # second hidden layer\n","        X = self.hidden2(X)\n","        X = self.act2(X)\n","        # output layer\n","        X = self.hidden3(X)\n","        return torch.softmax(X, dim=1)\n","        X = self.act3(X)\n","\n","        return X\n","\n","# prepare the dataset\n","def prepare_data(path):\n","    # load the dataset\n","    print(\"----- dans prepare data avant CSV Dataset\")\n","    dataset = CSVDataset(path)\n","    # calculate split\n","    train, test = dataset.get_splits()\n","    print (\"Taille du jeu de test et d'apprentissage\",len(train),len(test))\n","    # prepare data loaders\n","    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n","    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n","    return train_dl, test_dl\n","\n","# train the model\n","def train_model(train_dl, model):\n","    # define the optimization\n","    #criterion = BCELoss()\n","    # attention ici c'est une cross entropy\n","    criterion = CrossEntropyLoss()\n","    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n","    # enumerate epochs\n","    for epoch in range(EPOCHS):\n","        # enumerate mini batches\n","        for i, (inputs, targets) in enumerate(train_dl):\n","            \n","            # clear the gradients\n","            optimizer.zero_grad()\n","       \n","            # compute the model output\n","            yhat = model(inputs)#.numpy()\n","            \n","            # calculate loss\n","            loss = criterion(yhat, targets)\n","            \n","            # credit assignment\n","            loss.backward()\n","            # update model weights\n","            optimizer.step()\n","\n","# evaluate the model\n","def evaluate_model(test_dl, model):\n","    predictions, actuals = list(), list()\n","    for i, (inputs, targets) in enumerate(test_dl):\n","        # evaluate the model on the test set\n","        yhat = model(inputs)\n","        # retrieve numpy array\n","        yhat = yhat.detach().numpy()\n","        actual = targets.numpy()\n","        # convert to class labels\n","        yhat = argmax(yhat, axis=1)\n","        # reshape for stacking\n","        actual = actual.reshape((len(actual), 1))\n","        yhat = yhat.reshape((len(yhat), 1))\n","        # store\n","        predictions.append(yhat)\n","        actuals.append(actual)\n","    predictions, actuals = vstack(predictions), vstack(actuals)\n","    # calculate accuracy\n","    acc = accuracy_score(actuals, predictions)\n","    return acc\n","\n","# make a class prediction for one row of data\n","def predict(row, model):\n","    # convert row to data\n","    row = Tensor([row])\n","    # make prediction\n","    yhat = model(row)\n","    # retrieve numpy array\n","    yhat = yhat.detach().numpy()\n","    return yhat\n","\n","\n","# prepare the data\n","start_time = time.time()\n","path = 'ReviewsLabelled2.csv'\n","train_dl, test_dl = prepare_data(path)\n","\n","print(\"Len train_dl = nb batchsize\", len(train_dl), len(train_dl.dataset), len(test_dl.dataset),\"\\n\")\n","print(\"Prepare the data : %s seconds \\n\" % (time.time() - start_time))\n","# define the network\n","model = MLP(NB_FEATURES)\n","start_time = time.time()\n","# train the model\n","print(len(train_dl.dataset), len(test_dl.dataset))\n","#train_dl.describe(include = [object])\n","train_model(train_dl, model)\n","print(\"Train : %s seconds \\n\" % (time.time() - start_time))\n","\n","start_time = time.time()\n","# evaluate the model\n","acc = evaluate_model(test_dl, model)\n","print('Accuracy: %.3f' % acc)\n","print(\"Evaluate : %s seconds \\n\" % (time.time() - start_time))\n","# Creation du modele general sur toutes les donnée pour faire de la prédiction\n","\n","# NORMALEMENT ICI IL FAUT RELANCER L'APPRENTISSAGE SUR TOUT LE JEU DE DONNEES POUR AVOIR UN BON MODELE\n","\n","\n","# EN DESSOUS C'EST POUR MONTRER COMMENT ON PEUT UTILISER LE MODELE. JE N'AI PAS TROP TESTE. \n","# J'AI JUSTE VERIFIE QUE LES PRETRAIREMENTS ETAIENT BIEN FAIT : on nettoie et on fait le tf-idf avec le vocabulaire\n","# sur lequel on a appris\n","clean_data=pickle.load( open( \"vocabulaire.txt\", \"rb\" ) )\n","\n","\n","#je prepare The row que je vais lui passer\n","#text = [\"Good case, Excellent value\"]\n","#text_normalizer=TextNormalizer(lowercase=True,getstemmer=True,removestopwords=True,getlemmatisation=True,removedigit=True) \n","#text=text_normalizer.fit_transform(text)\n","#tfidf=TfidfVectorizer(vocabulary=clean_data)\n","#text=tfidf.fit_transform(text).toarray()\n","#print(text)\n","#yhat = predict(row, model)\n","#print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","#df = read_csv(path, header=None, sep='\\t', encoding='utf8')\n","\n","row = ['There is not a deal good enough that would drag me into that establishment again.']  #yeld\n","#df.values[2000:2006, :-1]\n","\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","row = row.astype('float32')\n","for i in range (0, len(row)):\n","  yhat = predict(row[0], model)\n","  print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","row = [\"Overall, I would recommend this phone over the new Walkman.\"] # Amazon\n","#df.values[2000:2006, :-1]\n","\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","row = row.astype('float32')\n","for i in range (0, len(row)):\n","  yhat = predict(row[0], model)\n","  print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","row = [\"I purchased this and within 2 days it was no longer working!!!!!!!!!\"] # Amazon\n","#df.values[2000:2006, :-1]\n","\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","row = row.astype('float32')\n","for i in range (0, len(row)):\n","  yhat = predict(row[0], model)\n","  print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","\n","row = [\"I think the most wonderful parts (literally, full of \"\"wonder\"\") are the excerpts from his works.  \"] #imdb\n","#df.values[2000:2006, :-1]\n","\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","row = row.astype('float32')\n","for i in range (0, len(row)):\n","  yhat = predict(row[0], model)\n","  print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_9Pixu42NVy","outputId":"3102a33a-6648-49d4-8a04-4157703f88a2","executionInfo":{"status":"ok","timestamp":1651020559801,"user_tz":-120,"elapsed":14600,"user":{"displayName":"Francisco Cardenas","userId":"06925703395123571947"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----- dans prepare data avant CSV Dataset\n","Taille du jeu de test et d'apprentissage 2011 990\n","Len train_dl = nb batchsize 63 2011 990\n","Prepare the data : 1.5746726989746094 seconds \n","\n","2011 990\n","Train : 12.264191150665283 seconds \n","\n","Accuracy: 0.800\n","Evaluate : 0.008051156997680664 seconds \n","\n","['there is not deal good enough that would drag me into that establishment again'] \n","\n","Predicted: [[2.7477548e-02 1.4466151e-04 2.2956135e-06 9.7237557e-01]] (class=3)\n","['overall would recommend this phone over the new walkman'] \n","\n","Predicted: [[1.0000000e+00 7.6456796e-09 3.5772047e-08 8.1461762e-09]] (class=0)\n","['i purchased this and within 2 days it was no longer working'] \n","\n","Predicted: [[9.9355054e-01 1.7775620e-03 1.0342069e-05 4.6616294e-03]] (class=0)\n","['i think the most wonderful parts literally full of wonder are the excerpts from his works'] \n","\n","Predicted: [[1.8176020e-04 9.9981815e-01 2.9544825e-08 6.6157739e-08]] (class=1)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:154: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"]}]},{"cell_type":"code","source":["# make a single prediction\n","row = ['There is not a deal good enough that would drag me into that establishment again.' , \"The food sucked, which we expected but it sucked more than we could have imagined.\" ] # Yeld\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","yhat = predict(row, model)\n","print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","# make a single prediction\n","row = [\"The mic is great.\"] #, 'I plugged it in only to find out not a darn thing worked.'] # Amazon\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","yhat = predict(row, model)\n","print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","row = ['There is not a deal good enough that would drag me into that establishment again.'] # , \"The food sucked, which we expected but it sucked more than we could have imagined.\" ] # Yeld\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","yhat = predict(row, model)\n","print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n","\n","# make a single prediction\n","row = [\"Overall, I would recommend this phone over the new Walkman.\", 'This is infuriating.'] # Amazon\n","row=TextNormalizer(lowercase=True).fit_transform(row)\n","#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n","print (row, '\\n')\n","vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n","\n","row=vectorizer2.fit_transform(row).toarray()\n","yhat = predict(row, model)\n","print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n"],"metadata":{"id":"1Bez3p-jWVsc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651020834017,"user_tz":-120,"elapsed":255,"user":{"displayName":"Francisco Cardenas","userId":"06925703395123571947"}},"outputId":"206cc2f1-a43d-4740-f667-b87fc77fa20a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['there is not deal good enough that would drag me into that establishment again', 'the food sucked which we expected but it sucked more than we could have imagined'] \n","\n","Predicted: [[[8.7326491e-01 9.9956721e-01 7.1431398e-01 2.5923661e-04]\n","  [1.2673506e-01 4.3276491e-04 2.8568596e-01 9.9974078e-01]]] (class=7)\n","['the mic is great'] \n","\n","Predicted: [[[1. 1. 1. 1.]]] (class=0)\n","['there is not deal good enough that would drag me into that establishment again'] \n","\n","Predicted: [[[1. 1. 1. 1.]]] (class=0)\n","['overall would recommend this phone over the new walkman', 'this is infuriating'] \n","\n","Predicted: [[[9.99882340e-01 3.59662389e-03 9.60669160e-01 1.15713347e-02]\n","  [1.17684205e-04 9.96403337e-01 3.93308140e-02 9.88428652e-01]]] (class=0)\n"]}]}]}