{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenzaLB/TER_Classification_Multitache/blob/main/opinion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FmbyiAS1nMF"
      },
      "outputs": [],
      "source": [
        " #les omports utilisés dans ce notebook \n",
        "import sys\n",
        "from numpy import vstack\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from  sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.pipeline import Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "matL-R30Drik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POHYbMKq_zNB"
      },
      "outputs": [],
      "source": [
        "my_local_drive='/content/gdrive/MyDrive/TER'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Classification binaire selon l'opinion**"
      ],
      "metadata": {
        "id": "lLq7zVJcdYge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Etape1 : Prétraitement du texte** \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Fonction MyCleanText **(TransformerMixin)**\n",
        "\n",
        "\n",
        "\n",
        "*   Mettre le texte en minuscule\n",
        "*   Se débarasser des stopwords\n",
        "*   Se débarasser des nombres\n",
        "*   Stemmatisation\n",
        "*   Lemmatisation\n",
        "\n",
        "\n",
        "> Fonction TextNormalizer \n",
        "\n",
        "\n",
        "*   Fit_transform de mon corpus propre\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UC_dVA7nDo81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#...................................................Fonction MyCleanText .............................................................................\n",
        "# mettre en minuscule\n",
        "#enlever les stopwords\n",
        "#se debarasser des nombres\n",
        "#stemmatisation\n",
        "#lemmatisation \n",
        "#......................................................................................................................................................\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def MyCleanText(X,\n",
        "                lowercase=False, #mettre en minuscule\n",
        "                removestopwords=False, #supprimer les stopwords\n",
        "                removedigit=False, #supprimer les nombres\n",
        "                getstemmer=False, #conserver la racine des termes\n",
        "                getlemmatisation=False #lemmatisation des termes\n",
        "                ):\n",
        "    sentence=str(X)\n",
        "    #suppression des caractères spéciaux\n",
        "    sentence = re.sub(r'[^\\w\\s]',' ', sentence)\n",
        "    # suppression de tous les caractères uniques\n",
        "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
        "    # substitution des espaces multiples par un seul espace\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
        "\n",
        "    # decoupage en mots\n",
        "    tokens = word_tokenize(sentence)\n",
        "    if lowercase:\n",
        "          tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # suppression ponctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [token.translate(table) for token in tokens]\n",
        "\n",
        "    # suppression des tokens non alphabetique ou numerique\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "    \n",
        "    # suppression des tokens numerique\n",
        "    if removedigit:\n",
        "        words = [word for word in words if not word.isdigit()]\n",
        "\n",
        "    # suppression des stopwords\n",
        "    if removestopwords:\n",
        "        words = [word for word in words if not word in stop_words]\n",
        "\n",
        "    # lemmatisation\n",
        "    if getlemmatisation:\n",
        "        lemmatizer=WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(word)for word in words]\n",
        "        \n",
        "\n",
        "    # racinisation\n",
        "    if getstemmer:\n",
        "        ps = PorterStemmer()\n",
        "        words=[ps.stem(word) for word in words]\n",
        "        \n",
        "    sentence= ' '.join(words)\n",
        "  \n",
        "    return sentence   "
      ],
      "metadata": {
        "id": "wGCMB0afGLa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#..................................................Etape 1 : prétraitement du texte ...................................................................................................\n",
        "#...................................................Fonction TextNormalizer  .............................................................................\n",
        "#fit_transform de mon corpus propre \n",
        "#......................................................................................................................................................\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, \n",
        "                 removestopwords=False, # suppression des stopwords\n",
        "                 lowercase=False,# passage en minuscule\n",
        "                 removedigit=False, # supprimer les nombres  \n",
        "                 getstemmer=False,# racinisation des termes \n",
        "                 getlemmatisation=False # lemmatisation des termes  \n",
        "                ):\n",
        "        \n",
        "        self.lowercase=lowercase\n",
        "        self.getstemmer=getstemmer\n",
        "        self.removestopwords=removestopwords\n",
        "        self.getlemmatisation=getlemmatisation\n",
        "        self.removedigit=removedigit\n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Nettoyage du texte\n",
        "        X=X.copy() # pour conserver le fichier d'origine\n",
        "        return [MyCleanText(text,lowercase=self.lowercase,\n",
        "                            getstemmer=self.getstemmer,\n",
        "                            removestopwords=self.removestopwords,\n",
        "                            getlemmatisation=self.getlemmatisation,\n",
        "                            removedigit=self.removedigit) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def fit_transform(self, X, y=None, **fit_params):\n",
        "        return self.fit(X).transform(X)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'lowercase':self.lowercase,\n",
        "            'getstemmer':self.getstemmer,\n",
        "            'removestopwords':self.removestopwords,\n",
        "            'getlemmatisation':self.getlemmatisation,\n",
        "            'removedigit':self.removedigit\n",
        "        }    \n",
        "    \n",
        "    def set_params (self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self,parameter,value)\n",
        "        return self    \n"
      ],
      "metadata": {
        "id": "hQju4_y6MnlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Etape 2 : Prepare the Data**\n",
        "* Load and prepare your data \n",
        "* Neural network models require numerical input\n",
        "* Data and numerical output data \n"
      ],
      "metadata": {
        "id": "4oaqnsS2FAgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z1NkC0S_QAx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"ReviewsLabelled.csv\", names=['sentence','sentiment','source'], header=0,sep='\\t', encoding='utf8')\n",
        "print(\"Echantillon de mon dataset \\n\")\n",
        "print(df.head())\n",
        "print(\"\\n\")\n",
        "print(\"Quelques informations importantes \\n\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Class :**\n",
        "\n",
        "* Constructor of the dataset object loads the data file .\n",
        "\n",
        "* the __len__() function can be used to get the number of rows of the dataset \n",
        "\n",
        "* the __get_item__() function is udes to get a specific sample by index\n",
        "\n",
        "* **The random_split** function can be used to split a dataset into train and test sets .Once split ,a selection of rows from the Dataset can be provided to a DataLoader , along with the batch size and wether the data should be shuffled every epoch .\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JB7Z3FLYGS3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#....................................... CSVDataset .............................................\n",
        "# j'obtiens une matrice avec de 0\n",
        "# utilisation TextNormalizer et Tfidf \n",
        "# c'est elle qui me parait la plus pertinente pour l'instant \n",
        "#...................................................................................................\n",
        "NB_FEATURES=1000\n",
        "EPOCHS=150\n",
        "\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "        # load the dataset\n",
        "        def __init__(self, path):\n",
        "            # load the csv file as a dataframe\n",
        "            df = read_csv(path,header=0, sep='\\t', encoding='utf8')\n",
        "            print(df.head())\n",
        "            # selection des données (inputs and outputs)\n",
        "            self.X = df.values[:, 0:1]\n",
        "            self.y = df.values[:, -2]\n",
        "            #création du pipeline \n",
        "            pipe = Pipeline([(\"cleaner\", TextNormalizer(lowercase=True)),\n",
        "                 (\"tfidf_vectorizer\", TfidfVectorizer(max_features=NB_FEATURES))])\n",
        "            self.X=pipe.fit_transform(self.X)\n",
        "            # sauvegarde du vocabulaire du TF_IDF pour tester d'autres données\n",
        "            pickle.dump(pipe.named_steps['tfidf_vectorizer'].vocabulary_,open(\"vocabulaire.txt\",\"wb\"))\n",
        "            # attention important il faut convertir le résultat de TF_IDF en array et dire que dedans ce \n",
        "            # sont des floats\n",
        "            self.X=self.X.toarray()\n",
        "            self.X = self.X.astype('float32')\n",
        "            # ne pas oublier de transformer le y via label encoder -> chaque classe aura un numéro (meme si je ne pense pas que ce soit utile dans le cas de l opinion)\n",
        "            #self.y = LabelEncoder().fit_transform(self.y)\n",
        "            ########################################### ANCIENNE VERSION ################################################################\n",
        "            #text_normalizer=TextNormalizer(lowercase=True,getstemmer=True,removestopwords=True,getlemmatisation=True,removedigit=True) \n",
        "            #appliquer fit.transform pour réaliser les pretraitements sur X \n",
        "            #self.X=text_normalizer.fit_transform(self.X)\n",
        "            #tfidf=TfidfVectorizer(ngram_range=(2, 2))\n",
        "            #self.X=tfidf.fit_transform(self.X).toarray()\n",
        "            #print(\"Mon nombre de colonnes est : \",self.X.shape[1])\n",
        "            #self.X=self.X.astype(\"float32\")\n",
        "            \n",
        "           \n",
        "            \n",
        "\n",
        "        # number of rows in the dataset\n",
        "        def __len__(self):\n",
        "             #return self.X.shape[1]\n",
        "             return len(self.X)\n",
        "\n",
        "        # get a row at an index\n",
        "        def __getitem__(self, idx):\n",
        "            return [self.X[idx], self.y[idx]]\n",
        "\n",
        "        # get indexes for train and test rows\n",
        "        def get_splits(self, n_test=0.30):\n",
        "        # determine sizes\n",
        "            test_size = round(n_test * len(self.X))\n",
        "            print(\"test_size\",test_size)\n",
        "            train_size = len(self.X) - test_size\n",
        "            print(\"train_size\",train_size)\n",
        "            # calculate the split\n",
        "            return random_split(self, [train_size, test_size])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8_rZr8xEhfOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "zT1DHZZ7S43z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"ReviewsLabelled.csv\"\n",
        "print(\"création de mon objet Dataset \\n\")\n",
        "dataset=CSVDataset(path)\n",
        "\n",
        "#probleme les modifications ne sont pas prises en compte ,mon texte comprte encore des majuscules etc ..\n",
        "print(\"Le set Obtenu \\n\")\n",
        "print(dataset.X)\n",
        "\n",
        "print(\"Le nombre de rows de mon Dataset :\\n\")\n",
        "print(dataset.__len__())\n",
        "\n",
        "print(\"le type de mon X \\n\")\n",
        "type(dataset.X)\n",
        "\n",
        "print(len(dataset.X))\n",
        "print(\"L'item à l'indice 100 \\n\")\n",
        "print(dataset.__getitem__(100))"
      ],
      "metadata": {
        "id": "dRujJf40HGKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**NB:**\n",
        "A DataLoader instance can be created for the training dataset ,testdataset and even the a validation dataset .\n"
      ],
      "metadata": {
        "id": "m_576-V_IiTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Etape 3 : Define the Model**\n",
        "To define a model in Pytorch we need to define a class that extends the Module Class\n",
        "\n",
        "* **The constructor :** defines the layers of the model.\n",
        "\n",
        "* **Forward Function :** defines how to forward propagate input through the defined layers of the model.\n",
        "\n",
        "**NB:** Many layers are available such as Linear for fully connected layers , Conv2d etc ..\n",
        "\n",
        "--> We are gonna use Linear \n"
      ],
      "metadata": {
        "id": "IOIFmBZ-JiZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Module):\n",
        "        # define model elements\n",
        "        def __init__(self,n_inputs ):\n",
        "            super(MLP, self).__init__()\n",
        "            # input to first hidden layer\n",
        "            self.hidden1 = Linear(n_inputs, 25) #13465\n",
        "            kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "            self.act1 = ReLU()\n",
        "            # second hidden layer\n",
        "            self.hidden2 = Linear(25, 20)\n",
        "            kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "            self.act2 = ReLU()\n",
        "            # third hidden layer and output\n",
        "            self.hidden3 = Linear(20, 1)\n",
        "            xavier_uniform_(self.hidden3.weight)\n",
        "            self.act3 = Sigmoid()\n",
        "\n",
        "        # forward propagate input\n",
        "        def forward(self, X):\n",
        "            # input to first hidden layer\n",
        "            X = self.hidden1(X)\n",
        "            X = self.act1(X)\n",
        "            # second hidden layer\n",
        "            X = self.hidden2(X)\n",
        "            X = self.act2(X)\n",
        "            # third hidden layer and output\n",
        "            X = self.hidden3(X)\n",
        "            X = self.act3(X)\n",
        "            return X\n"
      ],
      "metadata": {
        "id": "sWU8KNkPfHZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKfwiOXHoBCO"
      },
      "outputs": [],
      "source": [
        "# prepare the dataset\n",
        "def prepare_data(path):\n",
        "    # load the dataset\n",
        "    dataset = CSVDataset(path)\n",
        "    print(len(dataset.X))\n",
        "    # calculate split\n",
        "    train, test = dataset.get_splits()\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True) #remet shuffle a True\n",
        "    #shuffle=False veut dire que je decide de pas mélanger \n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
        "\n",
        "    return train_dl, test_dl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Etape 5 : Train the Model**\n",
        "The training process requires to define a loss function and an optimization algorithm .\n",
        "\n",
        "* **BCELoss** : for binary classification \n",
        "\n",
        "* CrossEntropyLoss : for multi-class classification  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tXZQaqAYK-_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A loop is required for the number of training epochs \n",
        "\n",
        "\n",
        "```\n",
        "for epoch in range (100)\n",
        "```\n",
        "\n",
        "Then an inner loop is required for the mini batches for the gradient descent\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for i,(inputs,targets) in enumerate(train_dl)\n",
        "```\n",
        "Each update to the model involves the same general pattern comprised of:\n",
        "\n",
        "* Clearing the last error gradient.\n",
        "\n",
        "* A forward pass of the input through the model.\n",
        "\n",
        "* Calculating the loss for the model output.\n",
        "\n",
        "* Backpropagating the error through the model.\n",
        "* Update the model in an effort to reduce loss.\n"
      ],
      "metadata": {
        "id": "rGwVKiufLwQS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MfYXT6toKRV"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from torch.nn.modules.loss import BCELoss\n",
        "def train_model(train_dl, model):\n",
        "            # define the optimization\n",
        "            criterion = BCELoss() #fonction cout \n",
        "            optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) #gradiant descent (momentum c'est une simple optimisation pour améliorer le learning rate )\n",
        "            # enumerate epochs\n",
        "            for epoch in range(500):\n",
        "                # enumerate mini batches\n",
        "                for i, (inputs, targets) in enumerate(train_dl):\n",
        "                #for data in DataLoader(train ,batch_size = 32 ,shuffle=True)\n",
        "                    targets=targets.unsqueeze(-1) #je l'ai mis car j'avais des erreurs de shape entre targets et inputs \n",
        "                    # clear the gradients\n",
        "                    optimizer.zero_grad()              \n",
        "                    # compute the model output\n",
        "                    yhat = model(inputs.float())\n",
        "                    # calculate loss\n",
        "                    loss = criterion(yhat, targets.float())\n",
        "                    # credit assignment\n",
        "                    loss.backward()\n",
        "                    # update model weights\n",
        "                    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Etape 6 : Evaluate the Model**"
      ],
      "metadata": {
        "id": "Z7VL57aENwce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CVo78u7oPwi"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model):\n",
        "                        predictions, actuals = list(), list()\n",
        "                        for i, (inputs, targets) in enumerate(test_dl):\n",
        "                            # evaluate the model on the test set\n",
        "                            yhat = model(inputs)\n",
        "                            # retrieve numpy array\n",
        "                            yhat = yhat.detach().numpy()\n",
        "                            actual = targets.numpy()\n",
        "                            actual = actual.reshape((len(actual), 1))\n",
        "                            # round to class values\n",
        "                            yhat = yhat.round()\n",
        "                            # store\n",
        "                            predictions.append(yhat)\n",
        "                            actuals.append(actual)\n",
        "                            predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "                            #print(predictions) -> je voulais juste voir si c'étaient reellement des 0 et des 1 -> c'est le cas\n",
        "                            # calculate accuracy\n",
        "                            acc = accuracy_score(actuals, predictions)\n",
        "                            return acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Etape 7: Make predictions**"
      ],
      "metadata": {
        "id": "-2bhDGRiN4Lw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cGLNGy2oV4-"
      },
      "outputs": [],
      "source": [
        "from nltk.sem.logic import Variable\n",
        "\n",
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "                # convert row to data\n",
        "                #row = Variable(Tensor([row]).float())\n",
        "                row=Tensor([row])\n",
        "                # make prediction\n",
        "                yhat = model(row)\n",
        "                # retrieve numpy array\n",
        "                yhat = yhat.detach().numpy()\n",
        "                return yhat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9WhYrFIogOx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# prepare data\n",
        "path = 'ReviewsLabelled.csv'\n",
        "train_dl, test_dl = prepare_data(path)\n",
        "print(\"Len train_dl = nb batchsize\", len(train_dl), len(train_dl.dataset), len(test_dl.dataset))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp8rUAFD-fbm"
      },
      "outputs": [],
      "source": [
        "\n",
        "#print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "# define the network\n",
        "nbrows=dataset.X.shape[1]\n",
        "model = MLP(nbrows)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "train_model(train_dl, model)"
      ],
      "metadata": {
        "id": "2aQMbPV5977d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "acc = evaluate_model(test_dl, model)\n",
        "print('Accuracy: %.3f' % acc)\n"
      ],
      "metadata": {
        "id": "tWxK7v6L95IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "metadata": {
        "id": "F2aTB286kpEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TESTS DE PREDICT***"
      ],
      "metadata": {
        "id": "xGe5d8s4XRYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation du modele general sur toutes les donnée pour faire de la prédiction\n",
        "\n",
        "# NORMALEMENT ICI IL FAUT RELANCER L'APPRENTISSAGE SUR TOUT LE JEU DE DONNEES POUR AVOIR UN BON MODELE\n",
        "\n",
        "\n",
        "# EN DESSOUS C'EST POUR MONTRER COMMENT ON PEUT UTILISER LE MODELE. JE N'AI PAS TROP TESTE. \n",
        "# J'AI JUSTE VERIFIE QUE LES PRETRAIREMENTS ETAIENT BIEN FAIT : on nettoie et on fait le tf-idf avec le vocabulaire\n",
        "# sur lequel on a appris\n",
        "clean_data=pickle.load( open( \"vocabulaire.txt\", \"rb\" ) )\n",
        "\n",
        "# make a single prediction\n",
        "row = [\"The mic is great.\"]\n",
        "#, \"The SKY IS BLUE\", 'my first visit to Hiro was a delight !'] # Amazon\n",
        "row=TextNormalizer(lowercase=True).fit_transform(row)\n",
        "#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n",
        "print (row, '\\n')\n",
        "vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n",
        "row=vectorizer2.fit_transform(row).toarray()\n",
        "yhat = predict(row, model)\n",
        "print('Predicted: %d' % round(yhat[0][0][0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "bF-cEMr4iHJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "row = [\"Instead, we got a bore fest about a whiny, spoiled brat babysitting.\"] #imdb\n",
        "row=TextNormalizer(lowercase=True).fit_transform(row)\n",
        "#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n",
        "print (row, '\\n')\n",
        "vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n",
        "\n",
        "row=vectorizer2.fit_transform(row).toarray()\n",
        "yhat = predict(row, model)\n",
        "#print(round(yhat[0][0][0]))\n",
        "print('Predicted: %d' % round(yhat[0][0][0]))\n",
        "\n",
        "#row = [My first visit to Hiro was a delight!] yeld"
      ],
      "metadata": {
        "id": "I1gd3lvZV49b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "row = [\"Good case, Excellent value\"] #imdb\n",
        "row=TextNormalizer(lowercase=True).fit_transform(row)\n",
        "#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n",
        "print (row, '\\n')\n",
        "vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n",
        "\n",
        "row=vectorizer2.fit_transform(row).toarray()\n",
        "yhat = predict(row, model)\n",
        "print('Predicted: %d' % round(yhat[0][0][0]))\n"
      ],
      "metadata": {
        "id": "IJufTYR8XZJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Classification multiclass selon la source**\n",
        "\n"
      ],
      "metadata": {
        "id": "rdJd57vjdPyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch mlp for multiclass classification\n",
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import sys\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn import BCELoss\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import torch\n",
        "from torch import nn\n",
        "import scipy\n",
        "from torch import optim\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def MyCleanText(X, \n",
        "               lowercase=False, # mettre en minuscule\n",
        "               removestopwords=False, # supprimer les stopwords\n",
        "               removedigit=False, # supprimer les nombres  \n",
        "               getstemmer=False, # conserver la racine des termes\n",
        "               getlemmatisation=False # lematisation des termes \n",
        "              ):\n",
        "    \n",
        "    sentence=str(X)\n",
        "\n",
        "    # suppression des caractères spéciaux\n",
        "    sentence = re.sub(r'[^\\w\\s]',' ', sentence)\n",
        "    # suppression de tous les caractères uniques\n",
        "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
        "    # substitution des espaces multiples par un seul espace\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
        "\n",
        "    # decoupage en mots\n",
        "    tokens = word_tokenize(sentence)\n",
        "    if lowercase:\n",
        "          tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # suppression ponctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [token.translate(table) for token in tokens]\n",
        "\n",
        "    # suppression des tokens non alphabetique ou numerique\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "    \n",
        "    # suppression des tokens numerique\n",
        "    if removedigit:\n",
        "        words = [word for word in words if not word.isdigit()]\n",
        "\n",
        "    # suppression des stopwords\n",
        "    if removestopwords:\n",
        "        words = [word for word in words if not word in stop_words]\n",
        "\n",
        "    # lemmatisation\n",
        "    if getlemmatisation:\n",
        "        lemmatizer=WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(word)for word in words]\n",
        "        \n",
        "\n",
        "    # racinisation\n",
        "    if getstemmer:\n",
        "        ps = PorterStemmer()\n",
        "        words=[ps.stem(word) for word in words]\n",
        "        \n",
        "    sentence= ' '.join(words)\n",
        "  \n",
        "    return sentence   \n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, \n",
        "                 removestopwords=False, # suppression des stopwords\n",
        "                 lowercase=False,# passage en minuscule\n",
        "                 removedigit=False, # supprimer les nombres  \n",
        "                 getstemmer=False,# racinisation des termes \n",
        "                 getlemmatisation=False # lemmatisation des termes  \n",
        "                ):\n",
        "        \n",
        "        self.lowercase=lowercase\n",
        "        self.getstemmer=getstemmer\n",
        "        self.removestopwords=removestopwords\n",
        "        self.getlemmatisation=getlemmatisation\n",
        "        self.removedigit=removedigit\n",
        "        #self.transform = transforms.Compose([transforms.ToTensor()])  \n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Nettoyage du texte\n",
        "        X=X.copy() # pour conserver le fichier d'origine\n",
        "        return [MyCleanText(text,lowercase=self.lowercase,\n",
        "                            getstemmer=self.getstemmer,\n",
        "                            removestopwords=self.removestopwords,\n",
        "                            getlemmatisation=self.getlemmatisation,\n",
        "                            removedigit=self.removedigit) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def fit_transform(self, X, y=None, **fit_params):\n",
        "        return self.fit(X).transform(X)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'lowercase':self.lowercase,\n",
        "            'getstemmer':self.getstemmer,\n",
        "            'removestopwords':self.removestopwords,\n",
        "            'getlemmatisation':self.getlemmatisation,\n",
        "            'removedigit':self.removedigit\n",
        "        }    \n",
        "    \n",
        "    def set_params (self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self,parameter,value)\n",
        "        return self    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w4aFj_PRdpc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xl6dPB8Id4UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_FEATURES=1000\n",
        "EPOCHS=150\n",
        "\n",
        "\n",
        "# dataset definition\n",
        "class CSVDataset(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, path):\n",
        "        # load the csv file as a dataframe\n",
        "        df = read_csv(path, header=None, sep='\\t', encoding='utf8')\n",
        "        #print(df.head())\n",
        "        # store the inputs and outputs\n",
        "        self.X = df.values[:, :-1]\n",
        "\n",
        "        self.y = df.values[:, -1]\n",
        "\n",
        "        # création du pipeline\n",
        "        pipe = Pipeline([(\"cleaner\", TextNormalizer(lowercase=True)),\n",
        "                 (\"tfidf_vectorizer\", TfidfVectorizer(max_features=NB_FEATURES))])\n",
        "\n",
        "\n",
        "        self.X=pipe.fit_transform(self.X)\n",
        "        # sauvegarde du vocabulaire du TF_IDF pour tester d'autres données\n",
        "        pickle.dump(pipe.named_steps['tfidf_vectorizer'].vocabulary_,open(\"vocabulaire.txt\",\"wb\"))\n",
        "\n",
        "        # attention important il faut convertir le résultat de TF_IDF en array et dire que dedans ce \n",
        "        # sont des floats\n",
        "        self.X=self.X.toarray()\n",
        "        self.X = self.X.astype('float32')\n",
        "        # ne pas oublier de transformer le y via label encoder -> chaque classe aura un numéro\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\n",
        "\n",
        "\n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        " \n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "       # return self.transform(self.x_data[idx]), self.transform(self.y_data[idx])\n",
        " \n",
        "    # get indexes for train and test rows\n",
        "    def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # calculate the split\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "# model definition\n",
        "class MLP(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs):\n",
        "        super(MLP, self).__init__()\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Linear(n_inputs, 10)\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Linear(10, 8)\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "        # third hidden layer and output\n",
        "        #self.hidden3 = Linear(8, 3)\n",
        "        self.hidden3 = Linear(8, 4)\n",
        "        xavier_uniform_(self.hidden3.weight)\n",
        "        #self.act3 = Softmax(dim=1)\n",
        "        self.act3 = Softmax(dim=1)\n",
        " \n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        # second hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        # output layer\n",
        "        X = self.hidden3(X)\n",
        "        return torch.softmax(X, dim=1)\n",
        "        X = self.act3(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "# prepare the dataset\n",
        "def prepare_data(path):\n",
        "    # load the dataset\n",
        "    print(\"----- dans prepare data avant CSV Dataset\")\n",
        "    dataset = CSVDataset(path)\n",
        "    # calculate split\n",
        "    train, test = dataset.get_splits()\n",
        "    print (\"Taille du jeu de test et d'apprentissage\",len(train),len(test))\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
        "    return train_dl, test_dl\n",
        "\n",
        "# train the model\n",
        "def train_model(train_dl, model):\n",
        "    # define the optimization\n",
        "    #criterion = BCELoss()\n",
        "    # attention ici c'est une cross entropy\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # enumerate epochs\n",
        "    for epoch in range(EPOCHS):\n",
        "        # enumerate mini batches\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            \n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "       \n",
        "            # compute the model output\n",
        "            yhat = model(inputs)#.numpy()\n",
        "            \n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, targets)\n",
        "            \n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        # evaluate the model on the test set\n",
        "        yhat = model(inputs)\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        # convert to class labels\n",
        "        yhat = argmax(yhat, axis=1)\n",
        "        # reshape for stacking\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc\n",
        "\n",
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "    # convert row to data\n",
        "    row = Tensor([row])\n",
        "    # make prediction\n",
        "    yhat = model(row)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat\n",
        "\n",
        "\n",
        "# prepare the data\n",
        "path = 'ReviewsLabelled.csv'\n",
        "train_dl, test_dl = prepare_data(path)\n",
        "\n",
        "print(\"Len train_dl = nb batchsize\", len(train_dl), len(train_dl.dataset), len(test_dl.dataset))\n",
        "\n",
        "# define the network\n",
        "model = MLP(NB_FEATURES)\n",
        "# train the model\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "#train_dl.describe(include = [object])\n",
        "train_model(train_dl, model)\n",
        "\n",
        "# evaluate the model\n",
        "acc = evaluate_model(test_dl, model)\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "CUnEwo-qd7Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Attention :le predict de la source est à régler !** "
      ],
      "metadata": {
        "id": "kCCd4p7PfBVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data=pickle.load( open( \"vocabulaire.txt\", \"rb\" ) )\n",
        "\n",
        "# make a single prediction\n",
        "#row = [\"The mic is great.\"]\n",
        "row=[\"The SKY IS BLUE, 'my first visit to Hiro was a delight !\"]\n",
        "#, \"The SKY IS BLUE\", 'my first visit to Hiro was a delight !'] # Amazon\n",
        "row=TextNormalizer(lowercase=True).fit_transform(row)\n",
        "#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n",
        "print (row, '\\n')\n",
        "vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n",
        "\n",
        "row=vectorizer2.fit_transform(row).toarray()\n",
        "yhat = predict(row, model)\n",
        "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n",
        "\n",
        "\n",
        "row = [\"Instead, we got a bore fest about a whiny, spoiled brat babysitting.\"] #imdb\n",
        "row=TextNormalizer(lowercase=True).fit_transform(row)\n",
        "#row=clean_data.named_steps[\"cleaner\"].fit_transform(row)\n",
        "print (row, '\\n')\n",
        "vectorizer2 = TfidfVectorizer(vocabulary=clean_data)\n",
        "\n",
        "row=vectorizer2.fit_transform(row).toarray()\n",
        "yhat = predict(row, model)\n",
        "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n",
        "\n",
        "#row = [My first visit to Hiro was a delight!] yeld\n"
      ],
      "metadata": {
        "id": "_Lt8gztzelOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MULTI LABEL CLASSIFICATION**\n"
      ],
      "metadata": {
        "id": "aNP7igd3gCSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature -> (une seul ,c'est le review )\n",
        "#labels -> on en a deux\n",
        "    #Opinion :binary \n",
        "    #Source :Multi Class\n"
      ],
      "metadata": {
        "id": "MYYU_XQthshL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "path ='ReviewsLabelled.csv'\n",
        "df=read_csv(path,header=0, sep='\\t', encoding='utf8')\n",
        "#print((df.head()))\n",
        "#print(df[:3000])\n",
        "print(\"The initial shape of my DataSet :\", df.shape ,\"\\n\")\n",
        "le=LabelEncoder()\n",
        "df.source=le.fit_transform(df.source)\n",
        "#print(\"\\n\")\n",
        "#print(\"Apres l'application du LABEL ENCODER Sur la SOURCE \\n\")\n",
        "#print(df[:3000])\n",
        "#amazon = 0 , imdb=1 et yelp=2\n",
        "#on veut d'abord faire que de la classification binaire \n",
        "#donc on va se passer de yelp pour l'instant \n",
        "\n",
        "df2=pd.DataFrame(df, columns =\n",
        "                 ['sentence','sentiment' ,'source' ])\n",
        "indexNames = df2[df2['source']== 2 ].index\n",
        "df2.drop(indexNames,inplace=True)\n",
        "\n",
        "print(\"The shape of My Dataset now that i kept only amazon and imdb dans source :\",df2.shape, \"\\n\")\n",
        "#print(df2[:3000])\n",
        "\n",
        "df2['target_list'] = df2[['sentiment', 'source']].values.tolist()\n",
        "\n",
        "final_df=df2[['sentence','target_list']].copy()\n",
        "print(final_df.head())\n",
        "#print(df2.head())\n",
        "\n",
        "labels=final_df.target_list\n",
        "print(labels)\n",
        "type(labels[0])\n"
      ],
      "metadata": {
        "id": "3XVFihfRiOtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "NB_FEATURES=1000\n",
        "EPOCHS=150\n",
        "\n",
        "\n",
        "class InitDataset(Dataset):\n",
        "        # load the dataset\n",
        "        def __init__(self, dataframe):\n",
        "            self.data=dataframe\n",
        "            self.feature=dataframe['sentence']\n",
        "            self.labels=self.data.target_list \n",
        "\n",
        "            #création du pipeline \n",
        "            pipe = Pipeline([(\"cleaner\", TextNormalizer(lowercase=True)),\n",
        "                 (\"tfidf_vectorizer\", TfidfVectorizer(max_features=NB_FEATURES))])\n",
        "            self.feature=pipe.fit_transform(self.feature)\n",
        "            # sauvegarde du vocabulaire du TF_IDF pour tester d'autres données\n",
        "            pickle.dump(pipe.named_steps['tfidf_vectorizer'].vocabulary_,open(\"MultiLabelvocabulaire.txt\",\"wb\"))\n",
        "            # attention important il faut convertir le résultat de TF_IDF en array et dire que dedans ce \n",
        "            # sont des floats\n",
        "            self.feature=self.feature.toarray()\n",
        "            self.feature = self.feature.astype('float32')\n",
        "             \n",
        "\n",
        "        # number of rows in the dataset\n",
        "        def __len__(self):\n",
        "             #return self.X.shape[1]\n",
        "             return len(self.feature)\n",
        "\n",
        "        # get a row at an index\n",
        "        def __getitem__(self, idx):\n",
        "            features=self.feature[idx,:]\n",
        "            labels=torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "            features=torch.tensor(features,dtype=torch.float32)\n",
        "            label1=torch.tensor(labels[0],dtype=torch.float32)\n",
        "            label2=torch.tensor(labels[1],dtype=torch.float32)\n",
        "\n",
        "            return {\n",
        "                'features ':features,\n",
        "                'label1':label1,\n",
        "                'label2':label2,\n",
        "            }\n",
        "\n",
        "        # get indexes for train and test rows\n",
        "        def get_splits(self, n_test=0.30):\n",
        "        # determine sizes\n",
        "            test_size = round(n_test * len(self.feature))\n",
        "            print(\"test_size\",test_size)\n",
        "            train_size = len(self.X) - test_size\n",
        "            print(\"train_size\",train_size)\n",
        "            # calculate the split\n",
        "            return random_split(self, [train_size, test_size])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#essais\n",
        "dataset=InitDataset(final_df)\n",
        "#print(dataset)\n",
        "dataset.__getitem__(4)\n"
      ],
      "metadata": {
        "id": "JEjhxrM9QNAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=df.sentence\n",
        "print(X)\n",
        "y=df.values[:,-2:]\n",
        "print(y[0])\n",
        "print(\"y\",y)\n",
        "Binarylabel=df.values[:,-2]\n",
        "print(Binarylabel)\n",
        "MultiLabel=df.values[:,-1]\n",
        "print(MultiLabel)\n"
      ],
      "metadata": {
        "id": "c0MH3UkKK6WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "VonoQKmIgJR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadBinaryModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadBinaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 32) # 1 is the number of features\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 256)\n",
        "        \n",
        "        # we will treat each head as a binary classifier ...\n",
        "        # ... so the output features will be 1\n",
        "        self.out1 = nn.Linear(256, 1)\n",
        "        self.out2 = nn.Linear(256, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        \n",
        "        # each binary classifier head will have its own output\n",
        "        out1 = F.sigmoid(self.out1(x))\n",
        "        out2 = F.sigmoid(self.out2(x))\n",
        "        \n",
        "        return out1, out2"
      ],
      "metadata": {
        "id": "Jxm7OMGKZQ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function for binary classification \n",
        "import torch.nn as nn\n",
        "\n",
        "# custom loss function for multi-head binary classification\n",
        "def binary_loss_fn(outputs, targets):\n",
        "    o1, o2= outputs\n",
        "    t1, t2= targets\n",
        "    l1 = nn.BCELoss()(o1, t1)\n",
        "    l2 = nn.BCELoss()(o2, t2)\n",
        "    return (l1 + l2 ) / 2"
      ],
      "metadata": {
        "id": "1xZDOySaZjR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.style.use('ggplot')"
      ],
      "metadata": {
        "id": "Tf360OTbZ9ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "train_dataset = final_df.sample(frac=train_size,random_state=200)\n",
        "valid_dataset = final_df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(final_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(valid_dataset.shape))\n",
        "\n",
        "training_set = InitDataset(train_dataset)\n",
        "validation_set = InitDataset(valid_dataset)\n",
        "\n",
        "#training data loader\n",
        "train_dataloader=DataLoader(training_set, shuffle=True, batch_size=1024)\n",
        "#initialize the model\n",
        "model=MultiHeadBinaryModel()"
      ],
      "metadata": {
        "id": "EottvpxPaN05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training function\n",
        "def train(model, dataloader, optimizer, loss_fn, train_dataset, device):\n",
        "    model.train()\n",
        "    counter = 0\n",
        "    train_running_loss = 0.0\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataset)/dataloader.batch_size)):\n",
        "        counter += 1\n",
        "        \n",
        "        # extract the features and labels\n",
        "        features = data['sentence'].to(device)\n",
        "        target1 = data['sentiment'].to(device)\n",
        "        target2 = data['source'].to(device)\n",
        "        \n",
        "        # zero-out the optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(features)\n",
        "        targets = (target1, target2)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        train_running_loss += loss.item()\n",
        "        \n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        # update optimizer parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "    train_loss = train_running_loss / counter\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "j3uIoaA8b9T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "############################################################################\n",
        "\n",
        "                                ESSAIS DIVERS \n",
        "                        \n",
        "############################################################################"
      ],
      "metadata": {
        "id": "KSSLkxUWXjfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#je prepare The row que je vais lui passer\n",
        "text = [\"Good case, Excellent value\"]\n",
        "text_normalizer=TextNormalizer(lowercase=True,getstemmer=True,removestopwords=True,getlemmatisation=True,removedigit=True) \n",
        "text=text_normalizer.fit_transform(text)\n",
        "tfidf=TfidfVectorizer(ngram_range=(2, 2))\n",
        "text=tfidf.fit_transform(text).toarray()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "YCdNO_oyJGhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "model_pipeline = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('model', model())]) #call the Model which you want to use\n",
        "\n",
        "model_pipeline.fit_transform(x,y) # here x is your text data, and y is going to be your target\n",
        "model_pipeline.predict(['Food was good\"'])  # predict your new sentence"
      ],
      "metadata": {
        "id": "RteftI1YY1pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = [\"I like the ambiance of this hotel \"]\n",
        "\n",
        "pd.DataFrame(tfidf_transformer.transform(count_vect.transform(new)).todense(), \n",
        "             columns = count_vect.get_feature_names())\n"
      ],
      "metadata": {
        "id": "9srjdXH_YVLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# make a single prediction (expect class=1)\n",
        "#c'est la ou je bloque , nb_inputs je l'ai mis en brut dans la definition de mon modele à 13..  , je dois trouver un moyen pour genéraliser \n",
        "#et peut être il faut changer la fonction predictit car la elle est supposé recevoir un commentaire , donc elle doit pouvoir faire les prétraitements toute seule \n",
        "\n",
        "#row =[0.57735027, 0.57735027, 0.57735027]\n",
        "#yhat=model.predict(text)\n",
        "\n",
        "yhat = predict(text, model)\n",
        "print('Predicted: (class=%d)' % (yhat.round()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rjle5cZNIs3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPKtWbnQAqWQ"
      },
      "outputs": [],
      "source": [
        "#...................................................................................................\n",
        "#............................................PREMIERS ESSAIS      ..................................\n",
        "#...................................................................................................\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "X=df.values[:,0:1]\n",
        "\n",
        "for phrase in X[:10,:] :\n",
        "  print(phrase)\n",
        "  phrase[0]= re.sub(\"[^a-zA-Z]\", \" \", phrase[0])\n",
        "  phrase[0]=phrase[0].lower()\n",
        "  phrase[0] = phrase[0].split()  # diviser enn entités\n",
        "  ps = PorterStemmer()\n",
        "  phrase[0]= [ps.stem(word) for word in phrase[0] if not word in set(stopwords.words(\"english\"))]  # tokenization et se debarasser des stopwords\n",
        "  phrase[0] = ' '.join(phrase[0])\n",
        "  print(phrase)\n",
        "  print(\"....................................\\n\")\n",
        "\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#...............................Troisieme VERSION DE CSVDataset ....................................\n",
        "#j'obtiens une matrice avec de 0\n",
        "#utilisation TextNormalizer et Tfidf \n",
        "#...................................................................................................\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "        # load the dataset\n",
        "        def __init__(self, path):\n",
        "            # load the csv file as a dataframe\n",
        "            df = read_csv(path,header=0, sep='\\t', encoding='utf8')\n",
        "            # selection des données\n",
        "            self.X = df.values[:, 0:1]\n",
        "            self.y = df.values[:, -2]\n",
        "            text_normalizer=TextNormalizer(lowercase=True,getstemmer=True,removestopwords=True,getlemmatisation=True,removedigit=True)  \n",
        "            # d'appliquer fit.transform pour appliquer les pré-traitements\n",
        "            self.X=text_normalizer.fit_transform(self.X)\n",
        "            print(\"X apres text_normalizer \\n\")\n",
        "            print(self.X)\n",
        "            # pour l'enchainer avec un tf-idf : \n",
        "            tfidf=TfidfVectorizer(ngram_range=(2, 2))\n",
        "            self.X=tfidf.fit_transform(self.X)\n",
        "            print(\"X apres tfidf  \\n\")\n",
        "            print(self.X)\n",
        "            # le vecteur peut par la suite être transformé en matrice : \n",
        "            self.X=self.X.toarray()\n",
        "            \n",
        "\n",
        "        # number of rows in the dataset\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "\n",
        "        # get a row at an index\n",
        "        def __getitem__(self, idx):\n",
        "            return [self.X[idx], self.y[idx]]\n",
        "\n",
        "        # get indexes for train and test rows\n",
        "        def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "            test_size = round(n_test * len(self.X))\n",
        "            train_size = len(self.X) - test_size\n",
        "            # calculate the split\n",
        "            return random_split(self, [train_size, test_size])\n",
        "\n",
        "path=\"ReviewsLabelled.csv\"\n",
        "dataset=CSVDataset(path)\n",
        "#probleme les modifications ne sont pas prises en compte ,mon texte comprte encore des majuscules etc ..\n",
        "print(\"Notre set final \\n\")\n",
        "print(dataset.X)\n",
        "print(\"La longueur de notre set final est :\\n\")\n",
        "print(dataset.__len__())\n"
      ],
      "metadata": {
        "id": "N2_iD61EnHFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH1AruVI88wb"
      },
      "outputs": [],
      "source": [
        "#...................................................................................................\n",
        "#...............................PREMIERE VERSION DE CSVDataset ....................................\n",
        "#j'obtiens une matrice avec de 0 (est ce que c'est normal ? )\n",
        "#utilisation de pipeline pour le prétraitement (classe TextNormaliser et Tfidf)\n",
        "#\n",
        "#...................................................................................................\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "        # load the dataset\n",
        "        def __init__(self, path):\n",
        "            # load the csv file as a dataframe\n",
        "            df = read_csv(path,header=0, sep='\\t', encoding='utf8')\n",
        "            # selection des données\n",
        "            self.X = df.values[:, 0:1]\n",
        "            self.y = df.values[:, -2]\n",
        "            # création du pipeline\n",
        "            pipe = Pipeline([(\"cleaner\", TextNormalizer()),\n",
        "                            (\"count_vectorizer\", TfidfVectorizer())])\n",
        "            pipe.fit(self.X)\n",
        "            pipe.transform(self.X)\n",
        "            # creation du dataframe pour affichage\n",
        "            df_pipe = pd.DataFrame(\n",
        "                data=pipe.transform(self.X).toarray(),\n",
        "                columns=pipe['count_vectorizer'].get_feature_names()\n",
        "            )\n",
        "            self.X=df_pipe\n",
        "\n",
        "            \n",
        "\n",
        "        # number of rows in the dataset\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "\n",
        "        # get a row at an index\n",
        "        def __getitem__(self, idx):\n",
        "            return [self.X[idx], self.y[idx]]\n",
        "\n",
        "        # get indexes for train and test rows\n",
        "        def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "            test_size = round(n_test * len(self.X))\n",
        "            train_size = len(self.X) - test_size\n",
        "            # calculate the split\n",
        "            return random_split(self, [train_size, test_size])\n",
        "\n",
        "\n",
        "\n",
        "path=\"ReviewsLabelled.csv\"\n",
        "dataset=CSVDataset(path)\n",
        "#probleme les modifications ne sont pas prises en compte ,mon texte comprte encore des majuscules etc ..\n",
        "print(\"Notre set final \\n\")\n",
        "print(dataset.X)\n",
        "print(\"La longueur de notre set final est :\\n\")\n",
        "print(dataset.__len__())\n",
        "\n",
        "print(\"le type de mon X \\n\")\n",
        "dataset.X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9FolQo7Dfy4"
      },
      "outputs": [],
      "source": [
        "# model definition\n",
        "\n",
        "class MLP(Module):\n",
        "        # define model elements\n",
        "        def __init__(self,n_inputs ):\n",
        "            super(MLP, self).__init__()\n",
        "            # input to first hidden layer\n",
        "            self.hidden1 = Linear(13465, 10)\n",
        "            kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "            self.act1 = ReLU()\n",
        "            # second hidden layer\n",
        "            self.hidden2 = Linear(10, 8)\n",
        "            kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "            self.act2 = ReLU()\n",
        "            # third hidden layer and output\n",
        "            self.hidden3 = Linear(8, 1)\n",
        "            xavier_uniform_(self.hidden3.weight)\n",
        "            self.act3 = Sigmoid()\n",
        "\n",
        "        # forward propagate input\n",
        "        def forward(self, X):\n",
        "            # input to first hidden layer\n",
        "            X = self.hidden1(X)\n",
        "            X = self.act1(X)\n",
        "            # second hidden layer\n",
        "            X = self.hidden2(X)\n",
        "            X = self.act2(X)\n",
        "            # third hidden layer and output\n",
        "            X = self.hidden3(X)\n",
        "            X = self.act3(X)\n",
        "            return X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model definition\n",
        "class MLP(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer = Linear(13465, 1)\n",
        "        self.activation = Sigmoid()\n",
        "\n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        X = self.layer(X)\n",
        "        X = self.activation(X)\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exemple d'utilisation \n",
        "\n",
        "#print ('Utilisation de MyCleanText avec convertion en minuscule, en prenant les racines, en supprimant les nombres')\n",
        "#print (MyCleanText(texte,lowercase=True,getstemmer=True, removedigit=True),'\\n')\n",
        "\n",
        "#print ('Utilisation de MyCleanText avec convertion en minuscule et en mettant sous la forme de lemmes')\n",
        "#print (MyCleanText(texte,lowercase=True,getlemmatisation=True),'\\n')"
      ],
      "metadata": {
        "id": "cyilWrGUItBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#...................................................................................................\n",
        "#...............................TEST DE LA CLASSE TEXTNORMALIZER....................................\n",
        "#...................................................................................................\n",
        "\n",
        "texte2 = [\"This is an example of TfidfVectorizer for creating a vector\",\n",
        "        \"This is another example of TfidfVectorizer\",\n",
        "        \"but before we apply a preprocessing\"]\n",
        "\n",
        "print (\"texte avant \",texte2)\n",
        "\n",
        "# il suffit de créer un objet de la classe TextNormalizer\n",
        "\n",
        "text_normalizer=TextNormalizer(lowercase=True)  \n",
        " \n",
        "# d'appliquer fit.transform pour appliquer les pré-traitements\n",
        "\n",
        "text_cleaned=text_normalizer.fit_transform(texte2)\n",
        "\n",
        "print (\"texte après application des pré-traitements\")\n",
        "\n",
        "print (text_cleaned)     \n",
        "\n",
        "# pour l'enchainer avec un tf-idf : \n",
        "tfidf=TfidfVectorizer(ngram_range=(2, 2))\n",
        "vector_tfidf=tfidf.fit_transform(text_cleaned)\n",
        "print (\"texte transformé en vecteur tf-idf\")\n",
        "print (vector_tfidf)\n",
        "\n",
        "# le vecteur peut par la suite être transformé en matrice : \n",
        "print (\"transformation du vecteur en matrice\")\n",
        "vector_tfidf.toarray()\n",
        "print(\"dimension : \\n\")\n",
        "\n",
        "# notons que cette matrice pourra être à l'entrée d'un classifier"
      ],
      "metadata": {
        "id": "oiQ29N2rOqYX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "opinion.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}